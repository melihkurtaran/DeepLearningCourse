{"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=view&id=1x-QAgitB-S5rxGGDqxsJ299ZQTfYtOhb\" width=180, align=\"center\"/>\n","\n","Master's degree in Intelligent Systems\n","\n","Subject: 11754 - Deep Learning\n","\n","Year: 2022-2023\n","\n","Professor: Miguel Ángel Calafat Torrens"],"metadata":{"id":"ukEDXM02WgLS"}},{"cell_type":"markdown","metadata":{"id":"_wjn76ZaN9K8"},"source":["# Lab 2 - The perceptron\n","\n","The perceptron is the smallest unit of neural network; that is, the one that only has one neuron. Its structure, as seen in the theoretical contents, is the one that appears below.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1as6Vm-uivPHatB_ly73LlxEr2n9wHPDw\" width=600>\n","\n","\n","See that the number of inputs represented corresponds to the number of parameters that are needed in the equation of a line in a two-dimensional space.\n","\n","$$ w_{1} x_{1} + w_{2} x_{2}+ w_{3}=0 $$"]},{"cell_type":"code","source":["# This cell connects to your Drive. This is necessary because we are going to\n","# import files from there\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"GKiR_y5SUtq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace the string in the following line with the path where you have this\n","# file. If you have your account in spanish, \"MyDrive\" is 'Mi unidad'.\n","%cd '/content/gdrive/MyDrive/Colab Notebooks/2022-2023-Lab.DL'\n","%ls -l"],"metadata":{"id":"hsBNWWkMe3Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here the path of the project folder (which is where this file is) is inserted\n","# into the python path. There's nothing to do; just execute the cell.\n","import pathlib\n","import sys\n","\n","PROJECT_DIR = str(pathlib.Path().resolve())\n","sys.path.append(PROJECT_DIR)"],"metadata":{"id":"-PXbtPyqfSaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FC9133BN9LA"},"outputs":[],"source":["# And here we import a few more libraries, among them the one for custom helper\n","# functions\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import helper_PR2 as hp\n","\n","# from ipywidgets import interact, interactive, fixed, interact_manual\n","# import ipywidgets as widgets\n","# from IPython.display import display"]},{"cell_type":"markdown","metadata":{"id":"vJKMfg78N9LC"},"source":["## Problem to solve\n","Imagine you have a cloud of dots. Each dot symbolizes two given characteristics of an individual that are to be used by a bank to determine whether to grant credit. The value of the abscissa could indicate the flow of monthly income, while the value of the ordinate would indicate the amount of money accumulated in a savings account. In this way, the position of the dots in the plane would determine a given pair of features of the economic situation of an individual.\n","\n","The following graph shows the dots that correspond to the last credit requests. The blue dots are accepted requests, while the red ones correspond to denied requests. It can be seen that in general, the larger the values of abscissa and ordinate, the more likely it is that credit will be granted, although this is not a rigid rule.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1B2LW8iyDCYVlOzYnCJ26dR0RYh2ZA01a\" width=\"400\" align=\"center\">\n","\n","If one wanted to assess the possibility of granting a loan based on a simple linear model (that is, with the equation of a line) that separated the data, one of the lines that would best fit the model would be the one shown below. .\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1uaPg44DatxAl9seD-fmCwpHhdQXLUW67\" width=\"400\" align=\"center\">\n","\n","In this way, although there would be exceptions, it could be said that the model generalizes reasonably well when it comes to predicting whether a loan will be granted to a person with given characteristics of monthly income and savings."]},{"cell_type":"markdown","metadata":{"id":"0Gm_jIhyN9LD"},"source":["Next, a model is going to be assembled to be able to fit any given set of points."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4XAdQ7BN9LD"},"outputs":[],"source":["# Seed for random numbers fixed to ensure reproducibility\n","np.random.seed(42)  # The answer to the great question of “life, the universe\n","                    # and everything” is 42, but you can choose any value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crivHtUhN9LE"},"outputs":[],"source":["# Generation of random dots with slight separation between groups\n","n = 20\n","\n","x1 = (0.4 * np.ones((1, n)) + 0.5 * np.random.random((1, n))).flatten()\n","y1 = (0.4 * np.ones((1, n)) + 0.5 * np.random.random((1, n))).flatten()\n","labels1 = n * [0]\n","\n","p1 = [(xs, ys) for xs, ys in zip(x1, y1)]\n","\n","x2 = (0.6 * np.ones((1, n)) + 0.5 * np.random.random((1, n))).flatten()\n","y2 = (0.6 * np.ones((1, n)) + 0.5 * np.random.random((1, n))).flatten()\n","labels2 = n * [1]\n","\n","p2 = [(xs, ys) for xs, ys in zip(x2, y2)]\n","\n","features = p1 + p2\n","correct_outputs = labels1 + labels2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nx7mPGOHN9LF"},"outputs":[],"source":["# Now change the values of weight1, weight2 and bias, and execute this cell\n","# Do it as many times as you consider necessary in order to understand how\n","# the parameters work. Try to find a combination of parameters\n","# that returns the best possible fit\n","\n","weight1 = 0.1\n","weight2 = 1.0\n","bias = -0.6\n","\n","hp.evaluate(weight1, weight2, bias, features, correct_outputs, extended=False)"]},{"cell_type":"markdown","metadata":{"id":"6TcfYRN5N9LG"},"source":["What you just did in the previous point is basically what the perceptron training algorithm is intended to do. That is, it is about starting from an initial value that gives a poorly adjusted model. Then, it has to modify the parameters to get closer to the result that is considered optimal.\n","\n","In this case, the way to do it is by observing the number of points that are well classified; however, a more objective and precise measure could be reached. Let's see how the calculations are made."]},{"cell_type":"markdown","metadata":{"id":"UVVAf87tN9LG"},"source":["## Forward pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFwsU36bN9LH"},"outputs":[],"source":["# It starts with some initial weights. For example:\n","weight1 = 0.1\n","weight2 = 1.0\n","bias = -0.6\n","\n","# Matrix of weights is called W\n","W = np.array([[weight1, weight2, bias]])\n","\n","# Dots are converted to an array\n","X = np.array(features)\n","\n","# The first point to be considered is selected.\n","x = X[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5lfEN0uN9LH"},"outputs":[],"source":["# Observe the dimensions of each array\n","print('Array W: {}\\nShape: {}\\n'.format(W, W.shape))\n","print('Array x:\\n{}\\nShape: {}\\n'.format(x, x.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuY8wOuzN9LI"},"outputs":[],"source":["# Note that with these dimensions you cannot perform\n","# the multiplication directly. They must be adapted first.\n","x = np.expand_dims(np.concatenate((x, np.array([1]))), axis=1)\n","print(x.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGhizgOPN9LJ"},"outputs":[],"source":["# Now you can multiply matrices directly\n","h = np.dot(W, x)\n","print(h)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spufJ2pLN9LJ"},"outputs":[],"source":["# In fact, this multiplication could be done by selecting several dots\n","# at once. For example, we want to select the first 10 dots\n","Xb = X[:10]\n","\n","# A vector of 1's is appended to it to be able to do matrix multiplication\n","Xb = np.concatenate((Xb.T, (np.array([len(Xb) * [1]]))), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeDhzr5QN9LK"},"outputs":[],"source":["# The matrix multiplication is performed and at this moment we already have\n","# the results of h for the first 10 dots\n","h = np.dot(W, Xb)\n","print(h)"]},{"cell_type":"markdown","metadata":{"id":"vNAaJJ4BN9LL"},"source":["Following the scheme in the figure below, now we just have to apply the activation function to obtain the first 10 values of ŷ.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1b_furS1eCvOlv947S8A69ErvzMbkDrWj\" width=\"600\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{"id":"2SLVssDCN9LM"},"source":["In this case the activation function will be a step function; that is, a function that returns the value 1 in case h is greater than or equal to zero, and returns zero otherwise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZ93WZrmN9LM"},"outputs":[],"source":["def stepFcn(h):\n","    out = np.zeros_like(h)\n","    out[h>=0] = 1\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuM40QjMN9LN"},"outputs":[],"source":["# The result is:\n","y_hat = stepFcn(h)\n","print(y_hat)\n","\n","# And it should have been:\n","print(correct_outputs[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVf2GooQN9LN"},"outputs":[],"source":["# Comparing the result obtained in the prediction with the ground truth\n","# you can know how many hits you have\n","print('You got {} out of {} wrong answers'.format(\n","     np.size(y_hat) - np.sum(y_hat == correct_outputs[:10]), np.size(y_hat)))"]},{"cell_type":"markdown","metadata":{"id":"NkQVlhxTN9LO"},"source":["Now that the _forward pass_ calculations have been done for a first batch of dots, let's see how we can compile it into functions for clarity."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cyPRnUTQN9LO"},"outputs":[],"source":["# The activation function will be the previously defined step function\n","activFcn = stepFcn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lunowkSMN9LO"},"outputs":[],"source":["def fwdPass(X, W, activFcn):\n","    # A vector of 1's is appended to be able to do matrix multiplication\n","    X = np.concatenate((X.T, (np.array([len(X) * [1]]))), axis=0)\n","    # Matrix multiplication is performed\n","    h = np.dot(W, X)\n","    # Activation function is applied\n","    y_hat = activFcn(h)\n","    \n","    return y_hat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9ee1r1pN9LP"},"outputs":[],"source":["# Note that in this way it can be applied to any length of the batch\n","print(fwdPass(X[:10], W, activFcn))\n","\n","print(fwdPass(X[:20], W, activFcn))\n","\n","# Or it can be done with all the dots at once\n","y_hat = fwdPass(X, W, activFcn)\n","print('You got {} out of {} wrong answers'.format(\n","    np.size(y_hat) - np.sum(y_hat == correct_outputs), np.size(y_hat)))"]},{"cell_type":"markdown","metadata":{"id":"_A6crOHMN9LP"},"source":["## Backpropagation\n","\n","Here comes the important point of training neural networks. At this point it is necessary to do the backward propagation, which is nothing more than updating the values of the weights in order to reduce the error (_loss_).\n","\n","When reviewing the formulas, it can be seen that derivatives are going to be used. For this reason, it is highly recommended that activation functions that can cause problems when deriving are not used (as is the case of the step function). So before we start with the first steps of back propagation, let's update the activation function. We will use the sigmoid function instead of the step function.\n","\n","Sigmoid function:\n","\n","$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CrASjVQXN9LQ"},"outputs":[],"source":["# Sigmoid function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# It is assigned as activation function\n","activFcn = sigmoid\n","\n","# A forward pass is performed with the new activation function\n","y_hat = fwdPass(X, W, activFcn)\n","\n","# Check for results\n","print('You got {} out of {} wrong answers'.format(\n","    np.size(y_hat) - np.sum(np.around(y_hat) == correct_outputs), np.size(y_hat)))"]},{"cell_type":"markdown","metadata":{"id":"3I_BcIYBN9LQ"},"source":["Until now, errors have been counted simply by seeing if a dot was or was not in the corresponding area; but now it is necessary to be more precise because derivatives are going to be used to calculate how much it is appropriate to vary the weights W.\n","\n","The first thing to do is define and calculate a loss function. In this case it will be calculated as the squared difference between the actual output and the estimated output. That is to say:\n","\n","$$ \\mathcal{loss}=\\frac{1}{2}\\sum_{i=1 }^{n}\\left ( y_i-\\hat{y}_i \\right )^{2} $$"]},{"cell_type":"markdown","metadata":{"id":"CyjQYbMyN9LQ"},"source":["The derivative of the loss function with respect to the weights W is what tells us how much the losses vary with the modification of the weights. In this case, we will have the following:\n","\n","$$ \\frac{\\partial loss}{\\partial W}=\\frac{\\partial loss}{\\partial \\hat{y}}·\\frac{\\partial \\hat{y}}{\\partial h}·\\frac{\\partial h}{\\partial W} $$\n","\n","The derivative of the activation function (in this case the sigmoid) is $f'(h)=\\sigma'(h)=\\sigma(h)\\,(1-\\sigma(h))$, therefore, it is equivalent to:\n","\n","$$ \\frac{\\partial loss}{\\partial W}=\\frac{\\partial loss}{\\partial \\hat{y}}·\\frac{\\partial \\hat{y}}{\\partial h}·\\frac{\\partial h}{\\partial W}=\n","\\overbrace{-(y-\\hat{y})}^{\\frac{\\partial loss}{\\partial \\hat{y}}} \\cdot \\overbrace{\\sigma(h)\\,(1-\\sigma(h))}^{\\frac{\\partial \\hat{y}}{\\partial h}} \\cdot \\overbrace{X}^{\\frac{\\partial h}{\\partial W}} $$\n","\n","The updating of the weights would be calculated in such a way that it is proportional to the derivative of the loss function. The proportionality factor is what we call _learning rate_, and it is represented by the Greek letter eta ($\\eta$):\n","\n","$$ \\Delta W = \\eta\\, \\frac{\\partial loss}{\\partial W} $$\n","\n","Finally, once the increments to be applied to the weights have been calculated, they are applied and the process starts again with the next epoch.\n","\n","$$ W^{(epoch+1)}=W^{(epoch)}-\\Delta W^{(epoch)}=W - \\eta\\, \\frac{\\partial loss}{\\partial W} = W + \\eta\\,  (y-\\hat{y}) \\cdot \\sigma(h)\\,(1-\\sigma(h)) \\cdot X   $$\n","\n","In some texts, the derivative of the loss function with respect to h is referred to with the lowercase letter delta ($\\delta$). In this way, the previous formula would be as follows:\n","\n","$$ W^{(epoch+1)}=W^{(epoch)}-\\Delta W^{(epoch)}=W - \\eta\\, \\delta\\, X   $$"]},{"cell_type":"markdown","source":["In short, the complete scheme would be as seen in the following figure:\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1eRjgfHYnWFPuFpD0CCWiVYbLqnwEUHzD\" width=\"600\" align=\"center\">\n"],"metadata":{"id":"vqG6WZk8oxlj"}},{"cell_type":"markdown","metadata":{"id":"yf1VUjvRN9LR"},"source":["## The training\n","\n","It's time for the first training. The following code is thoroughly commented. Pay particular attention to the dimensions of each array.\n","\n","In order to better emulate subsequent training, the dots will be delivered to the training algorithm in 10 at a time. Since there are 40 points in this example, this implies that we will have 4 batches in each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeVD8JqAN9LR"},"outputs":[],"source":["# Preparation data\n","\n","# Batch size\n","batch_size = 10\n","\n","# Initial values of the weights (3 x 1)\n","W = np.array([[weight1, weight2, bias]]).T\n","\n","# X is a 40 x 2 array of dots. In the first column are the abscissas and in the\n","# second are the ordinates; but now it will be delivered in batches, so it is\n","# convenient to have it sized in 4 batches. I mean:\n","# 4 batches x 10 dots x 2 coordinates.\n","X = np.array(features).reshape(4, -1, 2)\n","\n","# Following the same criteria, the correct labels will be\n","# arranged in a 4 x 40 x 1 array\n","Y = np.array([correct_outputs]).T.reshape(4, -1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPH3zFoHN9LR"},"outputs":[],"source":["# Training step\n","def trainStep(X, Y, W, lr):\n","    loss = 0.0\n","    for Xb, Yb in zip(X, Y):\n","        # Add the ones column to Xb (due to bias)\n","        # 10x3\n","        Xb = np.concatenate((Xb, np.ones((10, 1))), axis=1)\n","        \n","        # Forward pass\n","        # Calculate H. (10 x 3)·(3 x 1)==>(10 x 1)\n","        H = np.dot(Xb, W)\n","        # Activation function is applied. 10 x 1\n","        Yp = sigmoid(H)\n","        # Calculate the derivative of the activation function,\n","        # since it will be used in the backprop. It is dŶ/dh. 10 x 1\n","        dYp_dh = Yp * (1 - Yp)\n","        \n","        # Accumulated losses are calculated\n","        loss += np.square(Yb - Yp).sum()\n","\n","        # Backward pass\n","        # Calculate dloss/dỳ\n","        dloss_dYp = -(Yb - Yp)\n","\n","        # Delta error term is calculated. (10 x 1)\n","        delta = dloss_dYp * dYp_dh\n","        \n","        # The increments of W are calculated\n","        incW = lr * np.dot(Xb.T, delta)\n","        \n","        # Weights update\n","        W -= incW\n","        \n","    return W, loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2nuVGd99N9LR"},"outputs":[],"source":["# This cell executes a training step. Feel free to run it as many times as you\n","# wish, in order to observe how the weights and losses change with different\n","# values of lr (learning rate)\n","\n","# The learning rate is initialized\n","lr = 0.1\n","\n","W, loss = trainStep(X, Y, W, lr)\n","print('Loss: {}'.format(loss))\n","print('W:\\n{}\\n'.format(W))\n","\n","hp.evaluate(W[0], W[1], W[2], features, correct_outputs, extended=False)"]},{"cell_type":"markdown","metadata":{"id":"hhv02xdMN9LS"},"source":["Finally, look at what happens with different values of _lr_, whether they are large values or small values. You can also experiment with the number of epochs, and even with different starting values."]},{"cell_type":"code","source":["# Basic training of the network\n","# Hyperparameters' values\n","lr = 0.1\n","num_epochs = 800\n","\n","# Training\n","for epoch in range(num_epochs):\n","    W, loss = trainStep(X, Y, W, lr)\n","\n","# Results\n","print('Epoch: {} Loss: {}'.format(epoch, loss))\n","hp.evaluate(W[0], W[1], W[2], features, correct_outputs, extended=False)"],"metadata":{"id":"AFQPLGGJYK5z"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}